{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"C:/Users/PRADEEP REDDY/Desktop/IIITH-Summer Internship/Telugu-Question-Answering-master/Telugu-Question-Answering-master/Data/Questions/location.txt\", \"rb\")\n",
    "all_ques = []\n",
    "for q in file:\n",
    "    q = q.decode('utf-8').split('\\n')[0]\n",
    "    all_ques.append(q)\n",
    "file.close()\n",
    "#print(all_ques)\n",
    "#print(len(all_ques))\n",
    "################## Answers for the above questions ############\n",
    "import os\n",
    "\n",
    "for qu_in in range(len(all_ques)):\n",
    "    if os.path.exists('web.txt'):\n",
    "        os.remove('web.txt')\n",
    "    ###################################  Question Classification  #########################################\n",
    "    input_query = all_ques[qu_in] #input(\"Enter your Telugu Query for Question_Classification: \")\n",
    "    classes = []\n",
    "    questions = []\n",
    "    label_generalization = []\n",
    "    ans_type = ''\n",
    "    f = open(\"Data_for_Question_classification.txt\" , \"rb\")\n",
    "    for line in f:\n",
    "        #print(line.decode('utf-8'))\n",
    "        line = line.decode('utf-8').strip('\\n')\n",
    "        #print(line.split()[0])\n",
    "        label = (line.split()[0]).split(\":\")[0]    # accessing the labels for the questions in the data.\n",
    "        classes.append(label)\n",
    "        questions.append(line[6:])\n",
    "        #print(line[6:])\n",
    "        if(label==\"PERS\"):\n",
    "            label_generalization.append(1)         # Generalizing the classes\n",
    "        elif(label==\"LOCA\"):\n",
    "            label_generalization.append(2)\n",
    "        elif(label==\"ORGA\"):\n",
    "            label_generalization.append(3)\n",
    "        elif(label==\"DATE\"):\n",
    "            label_generalization.append(4)\n",
    "        elif(label==\"TIME\"):\n",
    "            label_generalization.append(5)\n",
    "        elif(label==\"PERC\"):\n",
    "            label_generalization.append(6)\n",
    "        elif(label==\"NUMB\"):\n",
    "            label_generalization.append(7)\n",
    "    #print(classes)\n",
    "    #print(len(classes))\n",
    "    #print(questions)\n",
    "    #print(len(questions))\n",
    "    #print(label_generalization)\n",
    "    #print(len(label_generalization))\n",
    "    \n",
    "    ################## Training and Testing phase #################\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    test_query = input_query\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    questions.append(test_query)\n",
    "    X = vectorizer.fit_transform(questions)\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    #print(X.shape)\n",
    "    \n",
    "    X1 = X[:len(label_generalization)]\n",
    "    Y1 = label_generalization[:len(label_generalization)]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, Y1, train_size = 0.8, test_size = 0.2, random_state = 48)\n",
    "    #print(X_test)\n",
    "    #print(y_test)\n",
    "    \n",
    "    classifier_MLP = MLPClassifier(hidden_layer_sizes = (100,80,40,7),activation = 'tanh',solver = 'adam',random_state = 3,alpha = 0.1,batch_size = 71)\n",
    "    #print(classifier_MLP)\n",
    "    \n",
    "    classifier_MLP.fit(X_train,y_train)\n",
    "    \n",
    "    test1_query = X[-1].toarray()\n",
    "    #print(X[-1])   # returns the doument_id, token_id, Tfidf score\n",
    "    #print(test1_query)\n",
    "    \n",
    "    prediction2 = classifier_MLP.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy: \",metrics.accuracy_score(y_test,prediction2)*100)\n",
    "    \n",
    "    ########################### Predicting the User Input query Class ##################################\n",
    "    print('\\n')\n",
    "\n",
    "    ip_query_predict = classifier_MLP.predict(test1_query)\n",
    "    if(ip_query_predict == 1):\n",
    "        predicted_output_query = \"PERSON\"\n",
    "    elif(ip_query_predict == 2):\n",
    "        predicted_output_query = \"LOCATION\"\n",
    "    elif(ip_query_predict == 3):\n",
    "        predicted_output_query = \"ORGANIZATION\"\n",
    "    elif(ip_query_predict == 4):\n",
    "        predicted_output_query = \"DATE\"\n",
    "    elif(ip_query_predict == 5):\n",
    "        predicted_output_query = \"TIME\"\n",
    "    elif(ip_query_predict == 6):\n",
    "        predicted_output_query = \"PERCENTAGE\"\n",
    "    else:\n",
    "        predicted_output_query = \"NUMBER\"\n",
    "            \n",
    "    #print(\"\\n%s belongs to the class [%s]\" %(test_query, predicted_output_query))\n",
    "        \n",
    "    ans_type += predicted_output_query\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    #################################### Translation of Telugu query to English ################################\n",
    "    from googletrans import Translator\n",
    "    trans = Translator()\n",
    "    en = trans.translate(input_query,dest = 'en',src = 'te')\n",
    "    tel_to_eng = en.text\n",
    "    \n",
    "    print(str(qu_in+1)+\": \"+\"Telugu to English: \", tel_to_eng)\n",
    "    \n",
    "    \n",
    "    ####################################  Getting the access to the web  #####################################\n",
    "    \n",
    "    import requests, webbrowser\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    query1 = tel_to_eng   #input(\"Enter your search query: \")\n",
    "    query = query1.lower()\n",
    "    search_engine1 = 'google' #input(\"Enter the search engine to search, \\\"Google\\\" or \\\"Bing\\\": \")\n",
    "    search_engine = search_engine1.lower()\n",
    "    no_of_urls = 6 #int(input(\"Enter the number of search results You required: \"))\n",
    "    \n",
    "    url_list=[]\n",
    "    \n",
    "    def access_web(search_engine,query,url_list):\n",
    "        res = requests.get('https://' + search_engine + '.com/search?q=' + query)\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "        \n",
    "        if (search_engine == 'google'):                    ####### Using Google search engine #######\n",
    "            search_results = soup.select('div#main > div > div > div > a')\n",
    "            for i in range(len(search_results)):\n",
    "                url_list.append(search_results[i].get('href'))\n",
    "                #webbrowser.open('https://' + search_engine + '.com' + search_results[i].get('href'))\n",
    "            #print(url_list)\n",
    "            \n",
    "        elif (search_engine == 'bing'):                   ####### Using Bing search engine ########\n",
    "            if not(soup.findAll('li', attrs={'class':'b_algo'})):\n",
    "                access_web(search_engine,query,url_list)\n",
    "            for i in soup.find_all('li', class_ = 'b_algo'):\n",
    "                j = i.find('h2')\n",
    "                for url in j.find_all('a', href=True):\n",
    "                    url_list.append(url['href'])\n",
    "                    #webbrowser.open('https://' + search_engine +'.com' + url['href'])\n",
    "            #print (url_list)\n",
    "        else:\n",
    "            print(\"Enter the search engines either 'Google' or 'Bing' only......\")\n",
    "        return url_list\n",
    "    \n",
    "    url_list = access_web(search_engine,query,url_list)\n",
    "    \n",
    "    ########################## Getting the Content from the web ##########################\n",
    "    Sentences=[]\n",
    "    url_list1 = url_list[:no_of_urls]\n",
    "    for i in range(no_of_urls):\n",
    "        try:\n",
    "            if (search_engine == 'google'):\n",
    "                res1 = requests.get('https://' + search_engine + '.com' + url_list1[i])\n",
    "                res1.raise_for_status()\n",
    "            else:\n",
    "                res1 = requests.get(url_list1[i])\n",
    "                res1.raise_for_status()\n",
    "            content = BeautifulSoup(res1.text,'lxml')\n",
    "            \n",
    "            \n",
    "            if (i==0):\n",
    "                f = open('web.txt', 'wb')\n",
    "                for para in content.findAll(['p']):\n",
    "                    paragraph = para.text\n",
    "                    Sentences.append(paragraph.strip())\n",
    "                    f.write(bytes(paragraph,'utf-8'))\n",
    "                f.close()\n",
    "            else:\n",
    "                f1 = open('web.txt', 'ab')\n",
    "                for para in content.findAll(['p']):\n",
    "                    paragraph = para.text\n",
    "                    Sentences.append(paragraph.strip())\n",
    "                    f1.write(bytes(paragraph,'utf-8'))\n",
    "                f1.close()\n",
    "        except requests.HTTPError as e:\n",
    "            print (e)\n",
    "        except IOError as e:\n",
    "            print(e)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #print(\" \")\n",
    "            #raise\n",
    "    #print(Sentences)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############################    Finding the similarity matrix  ##########################\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "    a = Sentences\n",
    "    universal_sentence = \"\"\n",
    "    all_tokens = []\n",
    "    for i in range(len(a)):\n",
    "        universal_sentence += (a[i] + ' ')\n",
    "        \n",
    "    One_sentence = sent_tokenize(universal_sentence)\n",
    "    for j in One_sentence:\n",
    "        all_tokens.append(word_tokenize(j))\n",
    "        \n",
    "    all_tokens_combined = word_tokenize(str.lower(universal_sentence))\n",
    "    all_unique_tokens_combined = list(set(all_tokens_combined))\n",
    "    \n",
    "    \n",
    "    ###################################   Integer Encoding of all sentences  ###########################################\n",
    "    \n",
    "    #print(all_unique_tokens_combined,\"\\n\\n\")\n",
    "    integer_encoding = {}\n",
    "    for i,word in enumerate(all_unique_tokens_combined):\n",
    "        \n",
    "        integer_encoding[word] = i\n",
    "    #print(\"\\n\\n------------------------------------Integer Encoding---------------------------------------\\n\\n\")\n",
    "    #print(\"Integer Encoded words:\\n\",integer_encoding)\n",
    "    \n",
    "    ################################  Integer encoding for each sentence  ########################################\n",
    "    \n",
    "    def integer_encoding1(subset_sentence):\n",
    "        subset_sentence_tokens = word_tokenize(subset_sentence)\n",
    "        subset_sentence_tokens_encoded = {}\n",
    "        for i in subset_sentence_tokens:\n",
    "            j = i\n",
    "            if (j not in integer_encoding.keys()):\n",
    "                j+='.'\n",
    "            if (j not in integer_encoding.keys()):\n",
    "                integer_encoding[j] = len(integer_encoding)\n",
    "            subset_sentence_tokens_encoded[j] = integer_encoding[j]\n",
    "            #subset_sentence_tokens_encoded.append(integer_encoding[i])\n",
    "        return (subset_sentence_tokens_encoded)\n",
    "    \n",
    "    #################################################  Cosine Similarity    #####################################################\n",
    "    \n",
    "    import math\n",
    "    def Cosine_Similarity(vec1, vec2):\n",
    "        common = set(vec1.keys()) & set(vec2.keys())\n",
    "        numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "        \n",
    "        sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "        sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "        denominator = (math.sqrt(sum1) * math.sqrt(sum2))\n",
    "        if not denominator:\n",
    "            return(0.0)\n",
    "        \n",
    "        else:\n",
    "            return(float(numerator) / denominator)\n",
    "        \n",
    "    text1 = query\n",
    "    ret_sent = {}\n",
    "    for k in range(len(One_sentence)):\n",
    "        text2 = str.lower(One_sentence[k])\n",
    "        vector1 = integer_encoding1(text1)\n",
    "        vector2 = integer_encoding1(text2)\n",
    "        cosine_similar = Cosine_Similarity(vector1, vector2)\n",
    "        ret_sent[One_sentence[k]] = cosine_similar\n",
    "    \n",
    "    #print(ret_sent)\n",
    "    \n",
    "    \n",
    "    ####################################### Picking Most relevant sentences ###################################################\n",
    "    \n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "    no_of_relevant_sentences = 30 #int(input(\"Enter the number of relevant sentences you require:\"))\n",
    "    def sort_dict(d):\n",
    "        return sorted(d.items(), key = lambda x : x[1], reverse = True)\n",
    "    most_relevant_sentences = sort_dict(ret_sent)\n",
    "    #print(most_relevant_sentences)\n",
    "    \n",
    "    for l in range(min(no_of_relevant_sentences,len(most_relevant_sentences))):\n",
    "        print(most_relevant_sentences[l][0],'\\n')\n",
    "        \n",
    "        \n",
    "    \n",
    "    ############################  NER Tagging #######################\n",
    "    print('\\n\\n\\t\\t\\t\\t--------NER Tagging starts------')\n",
    "    \n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    possible_entities_dictionary = {}\n",
    "    possible_entities_dictionary_modified = {}\n",
    "    \n",
    "    for j in range(min(no_of_relevant_sentences,len(most_relevant_sentences))):\n",
    "        entities = []\n",
    "        text = most_relevant_sentences[j][0]\n",
    "        doc = nlp(text)\n",
    "        #print(doc)\n",
    "        \n",
    "        for entity in doc.ents:\n",
    "            entities.append([entity.text, entity.label_])\n",
    "        #print(entities)\n",
    "        \n",
    "        for ne, label in entities:\n",
    "            if (label not in possible_entities_dictionary.keys()):\n",
    "                possible_entities_dictionary[label] = [ne]\n",
    "                \n",
    "            else:\n",
    "                possible_entities_dictionary[label].append(ne)\n",
    "        \n",
    "        for i in possible_entities_dictionary.keys():\n",
    "            if(i == 'CARDINAL'):\n",
    "                possible_entities_dictionary_modified['NUMBER'] = possible_entities_dictionary[i]\n",
    "            elif(i == 'GPE'):\n",
    "                possible_entities_dictionary_modified['LOCATION'] = possible_entities_dictionary[i]\n",
    "            elif(i == 'ORG'):\n",
    "                possible_entities_dictionary_modified['ORGANIZATION'] = possible_entities_dictionary[i]\n",
    "            elif(i == 'PERCENT'):\n",
    "                possible_entities_dictionary_modified['PERCENTAGE'] = possible_entities_dictionary[i]\n",
    "            else:\n",
    "                if(i in ['PERSON','DATE','TIME']):\n",
    "                    possible_entities_dictionary_modified[i] = possible_entities_dictionary[i]\n",
    "                    \n",
    "                    \n",
    "    print('\\n',possible_entities_dictionary,'\\n\\n')\n",
    "    \n",
    "    print('\\n-----------------------------Possible entities dictionary----------------------------\\n')\n",
    "    \n",
    "    print(possible_entities_dictionary_modified,'\\n\\n')\n",
    "    if (ans_type in possible_entities_dictionary_modified.keys()):\n",
    "        possible_answers = possible_entities_dictionary_modified[ans_type]\n",
    "    else:\n",
    "        possible_answers = ['no answer']\n",
    "    \n",
    "    print(\"Possible answers: \", possible_answers,\"\\n\\n\")\n",
    "\n",
    "    ####################### Converting Best answer to Telugu #########################\n",
    "    \n",
    "    from googletrans import Translator     # Works only with Internet\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import math\n",
    "    \n",
    "    english_query = tel_to_eng\n",
    "    possible_entities_dictionary1 = possible_entities_dictionary_modified\n",
    "    \n",
    "    all_possible_answers1 = possible_answers[:]\n",
    "    \n",
    "    token_words = word_tokenize(english_query.lower())\n",
    "    \n",
    "    translator = Translator()\n",
    "    \n",
    "    #print(token_words)\n",
    "    \n",
    "    \n",
    "    def Cosine_Similarity1(vec1, vec2):\n",
    "        common = set(vec1.keys()) & set(vec2.keys())\n",
    "        #print(common)\n",
    "        numerator = sum([vec1[x] * vec2[x] for x in common])\n",
    "        \n",
    "        sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "        sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "        denominator = (math.sqrt(sum1) * math.sqrt(sum2))\n",
    "        if not denominator:\n",
    "            return(0.0)\n",
    "        else:\n",
    "            return(float(numerator) / denominator)\n",
    "        \n",
    "    text1 = tel_to_eng\n",
    "    \n",
    "    for k in range(len(possible_answers)):\n",
    "        text2 = str.lower(possible_answers[k])\n",
    "        vector1 = integer_encoding1(text1.lower())\n",
    "        #print(vector1)\n",
    "        vector2 = integer_encoding1(text2)\n",
    "        #print(vector2)\n",
    "        cosine_similar1 = Cosine_Similarity1(vector1, vector2)\n",
    "        #print(cosine_similar1)\n",
    "        if (cosine_similar1 > 0.0):\n",
    "            all_possible_answers1.remove(possible_answers[k])\n",
    "            \n",
    "    print(all_possible_answers1)\n",
    "    \n",
    "    all_possible_answers = all_possible_answers1\n",
    "    \n",
    "    dict1 = {}\n",
    "    for i in all_possible_answers:\n",
    "        if i not in dict1.keys():\n",
    "            dict1[i] = 1\n",
    "        else:\n",
    "            dict1[i] += 1\n",
    "    #print(dict1)\n",
    "    if(len(dict1.values())>0):\n",
    "        max_count = max(dict1.values())\n",
    "        for k in dict1.keys():\n",
    "            if (dict1[k] == max_count):\n",
    "                Best_answer = k\n",
    "                break\n",
    "    else:\n",
    "        Best_answer = 'no answer'\n",
    "    \n",
    "    if(Best_answer.isnumeric()):\n",
    "        Best_answer1 = Best_answer\n",
    "        print(\"Ans: \",Best_answer1)\n",
    "        \n",
    "    else:\n",
    "        a = translator.translate(Best_answer, dest='te',src='auto')\n",
    "        #print(a)\n",
    "        Best_answer1 = a.text\n",
    "        print(\"Ans: \",Best_answer1)\n",
    "    \n",
    "    \n",
    "    que_trans = all_ques[qu_in]\n",
    "\n",
    "    if(qu_in == 0):\n",
    "        write_content = ans_type+':'+' '+que_trans[:]+\"@@@\"+Best_answer1+'\\n'\n",
    "        my_file = open(\"C:/Users/PRADEEP REDDY/Desktop/IIITH-Summer Internship/Open Domain Questoin Answering/Week-6/MLP/Location/MLP_loc_30_ans_pred.txt\",\"wb\")\n",
    "        my_file.write(bytes(write_content,'utf-8'))\n",
    "        print(write_content)\n",
    "    else:\n",
    "        write_content = ans_type+':'+' '+que_trans[:]+\"@@@\"+Best_answer1+'\\n'\n",
    "        my_file = open(\"C:/Users/PRADEEP REDDY/Desktop/IIITH-Summer Internship/Open Domain Questoin Answering/Week-6/MLP/Location/MLP_loc_30_ans_pred.txt\",\"ab\")\n",
    "        my_file.write(bytes(write_content,'utf-8'))\n",
    "        print(write_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
